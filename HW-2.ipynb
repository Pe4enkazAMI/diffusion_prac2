{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e4ca8ea-74b8-424f-b349-7b89a651a0a9",
   "metadata": {
    "id": "6e4ca8ea-74b8-424f-b349-7b89a651a0a9"
   },
   "source": [
    "# 1. Rectified Flow для спрямления траекторий диффузии"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b7f2e9-564f-4536-bc5c-b579b6255c88",
   "metadata": {},
   "source": [
    "В этой домашке мы имплементируем модель [Rectified Flow](https://arxiv.org/abs/2209.03003) для ускорения диффузионных моделей. Напомним, что модель предлагает обучать [Flow Matching](https://arxiv.org/abs/2210.02747) не на независимых парах (шум, данные), а на парах, полученных из предобученной диффузионной модели в ОДУ режиме. Такие пары имеют гораздо большую степень зависимости между собой, что позволяет более уверенно делать денойзинг из такой модели даже при больших уровнях шума. В теории же это означает, что траектории полученного ОДУ будут более прямыми, чем у оригинальной диффузионной модели (а чем прямее траектории, тем меньше шагов дискретизации нужно сделать для высокого качества генерации)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cf8a9f-b6af-4ed0-9be2-75d4223cbd12",
   "metadata": {
    "id": "84cf8a9f-b6af-4ed0-9be2-75d4223cbd12"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c651c37a-752f-4c02-9a51-6b26a28496a9",
   "metadata": {
    "id": "c651c37a-752f-4c02-9a51-6b26a28496a9"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Compose, Resize, ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0ba65d-c479-4ef4-a035-a92e8f735f74",
   "metadata": {
    "id": "1c0ba65d-c479-4ef4-a035-a92e8f735f74"
   },
   "source": [
    "## Цветной MNIST\n",
    "\n",
    "Как и прежде, модели будем запускать на цветной версии MNIST (код для покраски взят [у коллег](https://github.com/ngushchin/EntropicNeuralOptimalTransport/blob/main/src/tools.py) из Сколтеха)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba6253-ee32-4c0c-9015-4aed7cd61680",
   "metadata": {
    "id": "d5ba6253-ee32-4c0c-9015-4aed7cd61680"
   },
   "outputs": [],
   "source": [
    "class ColoredMNIST(MNIST):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hues = 360 * torch.rand(super().__len__())\n",
    "\n",
    "    def __len__(self):\n",
    "        return super().__len__()\n",
    "\n",
    "    def color_image(self, img, idx):\n",
    "        img_min = 0\n",
    "        a = (img - img_min) * (self.hues[idx] % 60) / 60\n",
    "        img_inc = a\n",
    "        img_dec = img - a\n",
    "\n",
    "        colored_image = torch.zeros((3, img.shape[1], img.shape[2]))\n",
    "        H_i = round(self.hues[idx].item() / 60) % 6\n",
    "\n",
    "        if H_i == 0:\n",
    "            colored_image[0] = img\n",
    "            colored_image[1] = img_inc\n",
    "            colored_image[2] = img_min\n",
    "        elif H_i == 1:\n",
    "            colored_image[0] = img_dec\n",
    "            colored_image[1] = img\n",
    "            colored_image[2] = img_min\n",
    "        elif H_i == 2:\n",
    "            colored_image[0] = img_min\n",
    "            colored_image[1] = img\n",
    "            colored_image[2] = img_inc\n",
    "        elif H_i == 3:\n",
    "            colored_image[0] = img_min\n",
    "            colored_image[1] = img_dec\n",
    "            colored_image[2] = img\n",
    "        elif H_i == 4:\n",
    "            colored_image[0] = img_inc\n",
    "            colored_image[1] = img_min\n",
    "            colored_image[2] = img\n",
    "        elif H_i == 5:\n",
    "            colored_image[0] = img\n",
    "            colored_image[1] = img_min\n",
    "            colored_image[2] = img_dec\n",
    "\n",
    "        return colored_image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = super().__getitem__(idx)\n",
    "        return self.color_image(img, idx), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659fca4-809c-4513-907b-6941c96f82df",
   "metadata": {
    "id": "9659fca4-809c-4513-907b-6941c96f82df"
   },
   "outputs": [],
   "source": [
    "transform = Compose([Resize((32, 32)), ToTensor()])\n",
    "data_train = ColoredMNIST(root='.', train=True, download=True, transform=transform)\n",
    "#data_train = ColoredMNIST(root='.', train=True, download=True, transform=transform)\n",
    "data_test = ColoredMNIST(root='.', train=False, download=False, transform=transform)\n",
    "train_dataloader = DataLoader(data_train, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(data_test, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f224c9-016a-4cdd-9bef-45ee4febc2be",
   "metadata": {
    "id": "10f224c9-016a-4cdd-9bef-45ee4febc2be"
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "def remove_ticks(ax):\n",
    "    ax.tick_params(\n",
    "        axis='both',\n",
    "        which='both',\n",
    "        bottom=False,\n",
    "        top=False,\n",
    "        labelbottom=False,\n",
    "        left=False,\n",
    "        labelleft=False\n",
    "    )\n",
    "\n",
    "def remove_xticks(ax):\n",
    "    ax.tick_params(\n",
    "        axis='both',\n",
    "        which='both',\n",
    "        bottom=False,\n",
    "        top=False,\n",
    "        labelbottom=False,\n",
    "        left=True,\n",
    "        labelleft=True\n",
    "    )\n",
    "\n",
    "def visualize_batch(img_vis, title='Семплы из цветного MNIST', nrow=10, ncol=4):\n",
    "    img_grid = make_grid(img_vis, nrow=nrow)\n",
    "    fig, ax = plt.subplots(1, figsize=(nrow, ncol))\n",
    "    remove_ticks(ax)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.imshow(img_grid.permute(1, 2, 0))\n",
    "    plt.show()\n",
    "\n",
    "visualize_batch(next(iter(train_dataloader))[0][:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b9e94f-e85d-4794-ba4a-d70bb019b4ea",
   "metadata": {
    "id": "33b9e94f-e85d-4794-ba4a-d70bb019b4ea"
   },
   "source": [
    "## Предобученная диффузионная модель\n",
    "\n",
    "Как и в первой домашке, мы будем использовать простенькую архитектуру, которая была получена скрещиванием CUNet из того же [репозитория](https://github.com/ngushchin/EntropicNeuralOptimalTransport/blob/main/src/cunet.py) и части, кодирующей момент времени и метку класса, из SongUNet в [EDM](https://github.com/NVlabs/edm/blob/main/training/networks.py). Далее идет повтор кода из предыдущей домашки для визуализации семплов из модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59ffa23-c0ee-4b2b-91af-b6f8eec8e5ac",
   "metadata": {
    "id": "c59ffa23-c0ee-4b2b-91af-b6f8eec8e5ac"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVlabs/edm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603e4374-54d7-4a4d-9b37-b280eb693c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('edm')\n",
    "sys.path.append('edm/training/')\n",
    "sys.path.append('edm/training/networks')\n",
    "\n",
    "import pickle\n",
    "from edm.dnnlib import util\n",
    "from torch_utils import misc\n",
    "from cunet import CUNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcc120d-3116-4856-b390-de9544c43e45",
   "metadata": {},
   "source": [
    "Далее мы будем работать не только с VE диффузией, но и с Flow Matching. Чтобы не переписывать один и тот же код несколько раз, заведем для диффузии и для FM по классу-обертке, которые помимо применения сети будут реализовывать подсчет векторного поля из соответствующего ОДУ и функцию, которая преобразовывает предсказание денойзера в это векторное поле."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de78bcda-b9e8-4337-9137-ad8a3bc1c860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# переопределим здесь EDMPrecond для простоты\n",
    "class EDMPrecond(nn.Module):\n",
    "\n",
    "    def __init__(self, model, sigma_data=0.5):\n",
    "        super().__init__()\n",
    "        self.sigma_data = sigma_data\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x, sigma, class_labels=None):\n",
    "        sigma = sigma.reshape(-1, 1, 1, 1)\n",
    "\n",
    "        c_skip = self.sigma_data ** 2 / (sigma ** 2 + self.sigma_data ** 2)\n",
    "        c_out = sigma * self.sigma_data / (sigma ** 2 + self.sigma_data ** 2).sqrt()\n",
    "        c_in = 1 / (self.sigma_data ** 2 + sigma ** 2).sqrt()\n",
    "        c_noise = sigma.log() / 4\n",
    "\n",
    "        F_x = self.model((c_in * x), c_noise.flatten(), class_labels=class_labels)\n",
    "        D_x = c_skip * x + c_out * F_x\n",
    "        return D_x\n",
    "\n",
    "\n",
    "class DiffODE(nn.Module):\n",
    "    def __init__(self, model, error_eps=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.error_eps = error_eps\n",
    "\n",
    "    def forward(self, x, sigma, class_labels=None):\n",
    "        return self.model(x, sigma, class_labels)\n",
    "\n",
    "    def velocity(self, x, sigma, class_labels=None):\n",
    "        sigma = sigma[:, None, None, None]\n",
    "        res = (-self.model(x, sigma, class_labels) + x) / (sigma + self.error_eps)\n",
    "        return res\n",
    "\n",
    "    def to_velocity(self, x, sigma, x_0):\n",
    "        sigma = sigma[:, None, None, None]\n",
    "        res = (-x_0 + x) / (sigma + self.error_eps)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25824ca2-7256-4c36-a1ec-de3bc211c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CUNet(in_channels=3, out_channels=3, noise_channels=128, base_factor=64, emb_channels=128)\n",
    "model = EDMPrecond(model)\n",
    "model.load_state_dict(torch.load('cunet.pt'))\n",
    "model = DiffODE(model)\n",
    "model.eval().cuda()\n",
    "\n",
    "print(f\"Модель имеет {sum(p.numel() for p in model.parameters())} параметров\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d027ab-1b5c-434b-9068-88f6a69b16b6",
   "metadata": {
    "id": "f2d027ab-1b5c-434b-9068-88f6a69b16b6"
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return x / x.abs().max(dim=0)[0][None, ...]\n",
    "\n",
    "def get_timesteps_diff(params):\n",
    "    num_steps = params['num_steps']\n",
    "    sigma_min, sigma_max = params['sigma_min'], params['sigma_max']\n",
    "    rho = params['rho']\n",
    "    step_indices = torch.arange(num_steps, device=params['device'])\n",
    "    t_steps = (sigma_max ** (1 / rho) + step_indices / (num_steps) * (sigma_min ** (1 / rho) - sigma_max ** (1 / rho))) ** rho\n",
    "    t_steps = torch.cat([t_steps, torch.zeros_like(t_steps[:1])]) # t_N = 0\n",
    "    return t_steps\n",
    "\n",
    "def sample_euler(model, noise, params, get_timesteps, class_labels=None, save_history=False, **model_kwargs):\n",
    "    num_steps = params['num_steps']\n",
    "    t_steps = get_timesteps(params)\n",
    "    x = noise\n",
    "\n",
    "    if save_history:\n",
    "        vis_steps = params['vis_steps']\n",
    "        x_history = [normalize(noise)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(t_steps) - 1):\n",
    "            t_cur = t_steps[i]\n",
    "            t_next = t_steps[i + 1]\n",
    "            t_net = t_steps[i] * torch.ones(x.shape[0], device=params['device'])\n",
    "            x = x + model.velocity(x, t_net) * (t_next - t_cur)\n",
    "            if save_history:\n",
    "                x_history.append(normalize(x).view(-1, 3, *x.shape[2:]))\n",
    "\n",
    "    if save_history:\n",
    "        x_history = [x_history[0]] + x_history[::-(num_steps // (vis_steps - 2))][::-1] + [x_history[-1]]\n",
    "        return x, x_history\n",
    "\n",
    "    return x, []\n",
    "\n",
    "def visualize_model_samples(model, params, get_timesteps, class_labels=None, title='Семплы из модели', diffusion=True, **model_kwargs):\n",
    "    if diffusion:\n",
    "        noise = torch.randn(40, 3, 32, 32).cuda() * params['sigma_max']\n",
    "    else:\n",
    "        noise = torch.randn(40, 3, 32, 32).cuda()\n",
    "    out, trajectory = sample_euler(model, noise, params, get_timesteps, class_labels=class_labels, **model_kwargs)\n",
    "    out = out * 0.5 + 0.5\n",
    "    visualize_batch(out.detach().cpu(), title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b82661-a653-46f6-a1bb-2b2920b27e41",
   "metadata": {
    "id": "f5b82661-a653-46f6-a1bb-2b2920b27e41"
   },
   "outputs": [],
   "source": [
    "sampling_params = {\n",
    "    'device': 'cuda',\n",
    "    'sigma_min': 0.02,\n",
    "    'sigma_max': 80.0,\n",
    "    'num_steps': 50,\n",
    "    'rho': 7.0,\n",
    "    'vis_steps': 1,\n",
    "    'stochastic': False\n",
    "}\n",
    "visualize_model_samples(model, params=sampling_params, get_timesteps=get_timesteps_diff, title='Семплы из предобученной модели')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be5575b-86fc-467c-b583-d0c8c27c33dc",
   "metadata": {},
   "source": [
    "Для исследования того, насколько Rectified Flow помогает сокращать число шагов по схеме Эйлера без потери качества, нам понадобится зависимость метрики качества генерации от количества итераций семплирования. Ниже изображен такой график для метрики FID предобученной модели (измеренный на 10000 семплах)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c4939b-0629-4f71-b79d-d5c81c6a15df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_model = np.array([194.77, 51.87, 27.73, 23.8732, 13.3967, 9.612, 6.71, 5.50, 4.976, 4.288, 3.59, 3.49, 3.265, 3.17, 2.85, 2.8745, 2.92, 2.588, 2.54, 2.55])\n",
    "\n",
    "def plot_fid(fid_model):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    for i in range(2):\n",
    "        ax[i].plot(np.arange(1, len(fid_model) + 1), fid_model, label='Pretrained model')\n",
    "        ax[i].plot(np.arange(0, len(fid_model) + 2), fid_model.min() * np.ones(len(fid_model) + 2),\n",
    "                   label='Best FID', linestyle='--', color='red', linewidth=1.0)\n",
    "        ax[i].grid(True)\n",
    "        ax[i].legend(fontsize=12)\n",
    "        ax[i].set_xlim([1, 21])\n",
    "        ax[i].set_xticks([1, 2, 3, 4, 5] + list(range(6, 21, 2)))\n",
    "        ax[i].tick_params(labelsize=12)\n",
    "        ax[i].set_xlabel('Euler iterations', fontsize=14)\n",
    "        ax[i].set_ylabel('FID', fontsize=14)\n",
    "\n",
    "    ax[0].annotate('FID = 2.54', (1.5, 3), fontsize=13)\n",
    "    ax[1].annotate('FID = 2.54', (1.5, 2.7), fontsize=13)\n",
    "    ax[0].set_title('Original scale', fontsize=15)\n",
    "    ax[0].set_ylim([1.5, 30.])\n",
    "    ax[1].set_title('Log scale', fontsize=15)\n",
    "    ax[1].set_yscale('log')\n",
    "    fig.suptitle('Num of Euler steps vs FID', fontsize=15)\n",
    "    plt.show()\n",
    "\n",
    "plot_fid(fid_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb21796-e681-43d9-95ae-cd89cf4bc9ce",
   "metadata": {},
   "source": [
    "## Модель Flow Matching\n",
    "\n",
    "Небольшое напоминание про модель Flow Matching [[1]](https://arxiv.org/abs/2210.02747)[[2]](https://arxiv.org/abs/2302.00482)[[3]](https://arxiv.org/abs/2303.08797): она учится переводить семплы из произвольного распределения $p_0$ в произвольное распределение $p_1$.  Мы считаем, что на этапе обучения нам приходят пары $(\\mathbf{X}_0, \\mathbf{X}_1)$ из некоторого совместного распределения $p_{0, 1}$, при этом $\\mathbf{X}_0 \\sim p_0$ и $\\mathbf{X}_1 \\sim p_1$. В данном случае $p_0 = \\mathcal{N}(0, I)$ — распределение шума, а $p_1 = p_{\\text{data}}$ — распределение данных. По парам строится процесс интерполяции $\\mathbf{X}_t = t \\mathbf{X}_1 + (1 - t) \\mathbf{X}_0$. На лекции мы доказывали, что если построить векторное поле\n",
    "$$\n",
    "    f_t^*(\\mathbf{x}) = \\mathbb{E}[\\mathbf{X}_1 - \\mathbf{X}_0 | \\mathbf{X}_t = \\mathbf{x}],\n",
    "$$\n",
    "то соответствующее ОДУ с начальным условием\n",
    "$$\n",
    "    \\begin{cases}\n",
    "        \\mathrm{d} \\mathbf{Y}_t = f_t(\\mathbf{Y}_t) \\mathrm{d} t;\\\\\n",
    "        \\mathbf{Y}_0 \\sim p_0\n",
    "    \\end{cases}\n",
    "$$\n",
    "порождает такую же динамику распределений, что и процесс интерполяции: $p_{\\mathbf{Y}_t} = p_{\\mathbf{X}_t}$ для всех $t$. В частности, данное ОДУ переводит семплы из распределения $p_0$ в распределение $p_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5409a26-d74b-4d53-9e13-e8332357deff",
   "metadata": {},
   "source": [
    "На практике, как обычно, мы пользуемся свойством УМО как наилучшего в $L_2$ предсказания и учим модель $f_t^{\\theta}(\\mathbf{x})$ на функционал\n",
    "$$\n",
    "    \\int_0^1 \\mathbb{E} \\|f_t^{\\theta}(\\mathbf{X}_t) - (\\mathbf{X}_1 - \\mathbf{X}_0) \\|^2 \\mathrm{d} t \\rightarrow \\min \\limits_{\\theta}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebc68d1-be69-47a0-9708-5f0ad3c548eb",
   "metadata": {},
   "source": [
    "Важнейшим свойством модели Flow Matching с линейным интерполянтом является то, что порожденное обученным ОДУ совместное распределение $p_{\\mathbf{Y}_0, \\mathbf{Y}_1}$ имеет меньшую транспортную цену, чем совместное распределение $p_{0, 1}$ пар, подаваемых во время обучения. Помимо снижения транспортной цены, обучение Flow Matching на семплах, полученных из некоторой ОДУ модели (например, из Flow Matching или диффузионной модели, обученной на независимых парах), приводит к уменьшению кривизны траекторий и, следовательно, к более эффективному семплингу. Так, в предельном случае, когда ОДУ имеет прямые траектории, генерацию можно сделать за один шаг по схеме Эйлера. Модель Flow Matching, использованную в таком сеттинге, принято называть Rectified Flow по названию оригинальной [статьи](https://arxiv.org/abs/2209.03003).\n",
    "\n",
    "Целью домашки будет обучение Rectified Flow и проверка того, действительно ли и насколько спрямляются траектории."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f5ddb4-0e75-4d41-9feb-caa8dee79ca3",
   "metadata": {},
   "source": [
    "## Использование диффузионной модели в режиме Flow Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a390e032-5bac-43dd-8086-96711b9ef1e5",
   "metadata": {},
   "source": [
    "В качестве исходной ОДУ модели, на парах из которой будет обучаться FM, мы возьмем предобученную диффузионную модель. При этом, если подумать, и исходная диффузионная модель, и будущий обученный FM будут решать одну и ту же задачу: предсказание шума/чистого семпла/разницы между шумом и чистым семплом по шумному входу. Логично в таком случае не обучать FM с нуля, а использовать предобученную диффузионную модель для его инициализации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0bb47e-813e-4744-b40f-e69040b29aae",
   "metadata": {},
   "source": [
    "## Задача 1 (0.2 балла)\n",
    "\n",
    "Пусть $D_\\sigma(\\mathbf{x}_\\sigma)$ — оптимальный денойзер, обученный в VE режиме, то есть $D_\\sigma(\\mathbf{x}_\\sigma) = \\mathbb{E}[\\mathbf{X} | \\mathbf{X} + \\sigma \\mathbf{\\varepsilon} = \\mathbf{x}_\\sigma]$, где $\\mathbf{X} \\sim p_{\\text{data}}$ и $\\mathbf{\\varepsilon} \\sim \\mathcal{N}(0, I)$ — независимые величины. Рассмотрим процесс интерполяции $\\mathbf{X}_t = t \\mathbf{X}_1 + (1 - t)\\mathbf{X}_0$, заданный моделью FM между $\\mathbf{X}_0 = \\mathbf{\\varepsilon}$ и $\\mathbf{X}_1 = \\mathbf{X}$.\n",
    "\n",
    "1) Выразите оптимальное векторное поле $f^*_t(\\mathbf{x}_t)$, выучиваемое моделью FM, через оптимальный денойзер для модели FM $D^*_t(\\mathbf{x}_t) = \\mathbb{E}[\\mathbf{X}_1 | \\mathbf{X}_t = \\mathbf{x}_t]$.\n",
    "\n",
    "2) Выразите оптимальный денойзер $D_t^*(\\mathbf{x}_t)$ для модели FM через оптимальный VE денойзер $D_\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e174ad85-c105-4042-9612-d032e6c1b2d3",
   "metadata": {},
   "source": [
    "#### Решение.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9146b5-e7a2-4703-b1c6-b84cf534ccd8",
   "metadata": {},
   "source": [
    "Решением задачи будет некоторое представление $D_t^*(\\mathbf{x}_t) = D_{\\sigma(t)}(\\varphi(\\mathbf{x}_t, t))$ и $f_t^*(\\mathbf{x}_t) = a(\\mathbf{x}_t, t) \\cdot D_t^*(\\mathbf{x}_t) + b(\\mathbf{x}_t, t)$. Новую модель Flow Matching мы будем параметризовать через FM-денойзер (и учить соответствующим образом), а инициализировать ее будем из VE-денойзера по полученным формулам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7ceaed-662c-45de-8e76-5e98aebdea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMPrecond(nn.Module):\n",
    "    def __init__(self, model, error_eps=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = model # VE-денойзер\n",
    "        self.error_eps = error_eps\n",
    "\n",
    "    # выражаем FM денойзер через VE денойзер и возвращаем предсказание чистой картинки\n",
    "    def forward(self, x_t, t, class_labels=None): \n",
    "        sigma = # sigma(t)\n",
    "        x_sigma = # phi(x_t, t)\n",
    "        return self.model(x_sigma, sigma, class_labels)\n",
    "\n",
    "    # выражаем векторное поле f*_t(x_t) через предсказание оптимального денойзера D*_t(x_t) (pred)\n",
    "    def to_velocity(self, x_t, t, denoiser_pred):\n",
    "        return # a(x_t, t) * denoiser_pred + b(x_t, t)\n",
    "\n",
    "    # комбинируем первое и второе\n",
    "    def velocity(self, x_t, t, class_labels=None):\n",
    "        return # a(x_t, t) * D*_t(x_t) + b(x_t, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067d9801-799e-4e2e-bb2b-f325d910e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# model: DiffODE, обертка над оберткой :)\n",
    "# model.model: EDMPrecond, обертка над нейросеткой, задающая VE денойзер\n",
    "\n",
    "recflow = FMPrecond(deepcopy(model.model))\n",
    "recflow.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01ded9a-795c-4c30-b1f4-df0423a0475d",
   "metadata": {},
   "source": [
    "Помимо самой предобученной модели и ее параметризации, преимуществом сведения FM к VE-денойзеру является возможность использовать все гиперпараметры, которые хорошо работали для исходной модели. В частности, это касается выбора моментов времени в качестве узлов для схемы Эйлера. Если все было реализовано правильно, должны получиться семплы, похожие на исходные семплы из модели (так как по сути мы использовали ту же модель, но с другим семплером, изначально разработанным для FM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69f02e2-95bb-4ef8-bb4b-49eea6726734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timesteps_fm(params):\n",
    "    sigmas = get_timesteps_diff(params)\n",
    "    return # здесь должна быть обратная к sigma(t) функция\n",
    "\n",
    "sampling_params = {\n",
    "    'device': 'cuda',\n",
    "    'sigma_min': 0.02,\n",
    "    'sigma_max': 80.0,\n",
    "    'num_steps': 20, \n",
    "    'rho': 7.0,\n",
    "    'vis_steps': 1,\n",
    "    'stochastic': False\n",
    "}\n",
    "\n",
    "visualize_model_samples(recflow, params=sampling_params, get_timesteps=get_timesteps_fm, diffusion=False, title='Семплы из инициализации FM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb133fe4-b8e9-49e0-b269-c04a7401a4b9",
   "metadata": {},
   "source": [
    "## Задача 2 (0.2 балла)\n",
    "Имплементируйте функционал обучения модели Rectified Flow на семплах из предобученной диффузионной модели и обучите модель. Как было обсуждено ранее, параметризуем мы модель через денойзер, поэтому и учить здесь будем на функционал денойзера (предсказание чистой картинки по шумной интерполяции).\n",
    "\n",
    "Обратите внимание, что VE модель начинает семплирование из $\\mathbf{\\varepsilon} \\cdot \\sigma_{\\text{max}}$ (здесь код для схемы Эйлера не производит умножение на $\\sigma_{\\text{max}}$, чтобы был общий код для FM и для VE). Помимо этого, sample_euler принимает на вход функцию, которая задает узлы дискретизации (get_timesteps_diff для диффузионной модели и get_timesteps_fm для FM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd5db8a-1181-4a53-99db-d1c15b327f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RectifiedLoss:\n",
    "    def __init__(self, diffusion, sampling_params, batch_dim, distr='rand'):\n",
    "        self.diffusion = diffusion # предобученная диффузионная модель\n",
    "        self.sampling_params = sampling_params # параметры семплирования из предобученной модели\n",
    "        self.batch_dim = batch_dim # размерность одного батча на обучении\n",
    "        self.distr = distr # распределение для семплирования моментов времени\n",
    "\n",
    "    def sample_t(self, n_samples, device='cuda'):\n",
    "        if self.distr == 'rand':\n",
    "            return torch.rand(n_samples, device=device)\n",
    "\n",
    "        if self.distr == 'logit':\n",
    "            n = torch.randn(n_samples, device=device)\n",
    "            return torch.sigmoid(n)\n",
    "\n",
    "    def __call__(self, net):\n",
    "        noise = torch.randn(self.batch_dim, device=self.sampling_params['device'])\n",
    "        images = ...\n",
    "        t = ...\n",
    "        x_t = ...\n",
    "        denoiser_pred = ...\n",
    "        loss = ...\n",
    "        log_imgs = {\n",
    "            'noise': noise.cpu().detach(),\n",
    "            'images': images.cpu().detach(),\n",
    "            'x_t': x_t.cpu().detach(),\n",
    "            'denoised': denoiser_pred.cpu().detach()\n",
    "        }\n",
    "\n",
    "        return loss, log_imgs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12301af1-37e5-4a31-b55f-402c73084851",
   "metadata": {},
   "source": [
    "Как и ранее, для удобства можно пользоваться следующим кодом для обучения + визуализации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ed0081-3262-4f79-a305-2c37269af853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "import os\n",
    "\n",
    "def visualize_mappings(log_imgs, ax, n_pictures=4):\n",
    "    img_vis = torch.cat((\n",
    "            normalize(log_imgs['noise'])[:n_pictures],\n",
    "            normalize(log_imgs['images'])[:n_pictures],\n",
    "            normalize(log_imgs['x_t'])[:n_pictures],\n",
    "            normalize(log_imgs['denoised'])[:n_pictures]),\n",
    "        dim=0\n",
    "    )\n",
    "    img_grid = make_grid(img_vis, nrow=n_pictures) * 0.5 + 0.5\n",
    "    ax.imshow(img_grid.permute(1, 2, 0).detach().cpu())\n",
    "\n",
    "def visualize_training(recflow, diffusion, loss_history, log_imgs, sampling_params, n_pictures=4, n_pictures_sampling=2):\n",
    "    fig, ax = plt.subplot_mosaic([['loss', 'denoising'],\n",
    "                                   ['sampling', 'sampling']],\n",
    "                                  figsize=(11, 9), layout=\"constrained\")\n",
    "\n",
    "    # loss visualization\n",
    "    ax['loss'].plot(np.arange(len(loss_history)), loss_history)\n",
    "    ax['loss'].grid(True)\n",
    "    ax['loss'].set_title('Лосс на обучении', fontsize=17)\n",
    "    ax['loss'].set_xlabel('Итерация', fontsize=14)\n",
    "    ax['loss'].tick_params(labelsize=13)\n",
    "\n",
    "    #denoising visualization\n",
    "    remove_ticks(ax['denoising'])\n",
    "    visualize_mappings(log_imgs, ax['denoising'], n_pictures=n_pictures)\n",
    "    ax['denoising'].set_title('Денойзинг из модели', fontsize=17)\n",
    "\n",
    "    #sampling\n",
    "    noise = torch.randn_like(log_imgs['images'][:n_pictures_sampling]).cuda()\n",
    "\n",
    "    remove_xticks(ax['sampling'])\n",
    "    _, trajectory = sample_euler(\n",
    "        diffusion, noise * sampling_params['sigma_max'], params=sampling_params, get_timesteps=get_timesteps_diff, save_history=True)\n",
    "\n",
    "    _, rtrajectory = sample_euler(\n",
    "        recflow, noise, params=sampling_params, get_timesteps=get_timesteps_fm, save_history=True)\n",
    "\n",
    "    trajectory = torch.cat(trajectory, dim=0) * 0.5 + 0.5\n",
    "    trajectory = trajectory.reshape(-1, n_pictures_sampling, *trajectory.shape[1:])\n",
    "    rtrajectory = torch.cat(rtrajectory, dim=0) * 0.5 + 0.5\n",
    "    rtrajectory = rtrajectory.reshape(-1, n_pictures_sampling, *rtrajectory.shape[1:])\n",
    "\n",
    "    trajectory = torch.cat((trajectory, rtrajectory), dim=1).reshape(-1, *trajectory.shape[2:])\n",
    "    n_pictures_sampling = n_pictures_sampling * 2\n",
    "    trajectory = trajectory.reshape(len(trajectory) // n_pictures_sampling, n_pictures_sampling, *trajectory.shape[-3:]).permute(1, 0, 2, 3, 4).reshape(-1, *trajectory.shape[-3:])\n",
    "    img_grid = make_grid(trajectory, nrow=len(trajectory) // n_pictures_sampling)\n",
    "\n",
    "    #sampling visualization\n",
    "    ax['sampling'].set_yticks(\n",
    "        18 + 34 * np.arange(n_pictures_sampling),\n",
    "        ['diffusion',] * (n_pictures_sampling // 2) + ['recflow',] * (n_pictures_sampling // 2)\n",
    "    )\n",
    "    ax['sampling'].imshow(img_grid.permute(1, 2, 0).detach().cpu())\n",
    "    ax['sampling'].set_title('Семплы из модели', fontsize=17)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def train(recflow, opt, loss_fn, n_iters, sampling_params, eval_every=100, save_every=1000,):\n",
    "    loss_history = []\n",
    "\n",
    "    with tqdm(total=n_iters) as pbar:\n",
    "        for it in range(n_iters):\n",
    "            opt.zero_grad()\n",
    "            loss, log_imgs = loss_fn(recflow)\n",
    "            loss.backward()\n",
    "            loss_history.append(loss.item())\n",
    "            opt.step()\n",
    "        \n",
    "            if it % eval_every == 0:\n",
    "                recflow.eval()\n",
    "                clear_output(wait=True)\n",
    "                visualize_training(recflow, loss_fn.diffusion, loss_history, log_imgs, sampling_params)\n",
    "\n",
    "            if it % save_every == 0:\n",
    "                torch.save(recflow.state_dict(), os.path.join('checkpoints', 'rec_%d.pth' %(it,)))\n",
    "            pbar.update(1)\n",
    "            pbar.set_description('Loss: %.4g' % loss.item())\n",
    "\n",
    "    return recflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e42954-bc5e-4569-8824-b1c412b96182",
   "metadata": {},
   "outputs": [],
   "source": [
    "recflow.train()\n",
    "opt = torch.optim.Adam(recflow.parameters(), lr=1e-5)\n",
    "model.eval().requires_grad_(False)\n",
    "\n",
    "# мы будем использовать одинаковые параметры для семплинга из\n",
    "# предобученной диффузионки и обучаемого Rectified Flow\n",
    "\n",
    "sampling_params = {\n",
    "    'device': 'cuda',\n",
    "    'sigma_min': 0.02,\n",
    "    'sigma_max': 80.0,\n",
    "    'num_steps': 20, # на 20 итерациях FID уже практически сошелся, больше будет слишком долго учиться\n",
    "    'vis_steps': 10,\n",
    "    'rho': 7.0,\n",
    "    'stochastic': False,\n",
    "}\n",
    "\n",
    "batch_dim = [64, 3, 32, 32] # можно попробовать и больший размер батча\n",
    "loss_fn = RectifiedLoss(model, sampling_params, batch_dim, distr='logit')\n",
    "n_iters = 15000 # может варьироваться в зависимости от размера батча"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b15360d-dc97-4eb3-bec7-9ae6f43ebb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "student = train(recflow, opt, loss_fn, n_iters, sampling_params, eval_every=100, save_every=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5549a0-508b-4805-be05-75d4450d8cdc",
   "metadata": {},
   "source": [
    "## Задача 3 (0.4 балла)\n",
    "\n",
    "**(0.1 балла)** Обучите модель Rectified Flow.\n",
    "\n",
    "Для отслеживания сходимости можно (но не требуется) добавить к обучению подсчет метрики кривизны траектории, которая для ОДУ $\\mathrm{d} \\mathbf{Y}_t = f_t(\\mathbf{Y}_t) \\mathrm{d} t$ определяется как\n",
    "$$\n",
    "    l(\\mathbf{Y}) = \\int\\limits_{0}^{1} \\mathbb{E} \\|f_t(\\mathbf{Y}_t) - (\\mathbf{Y}_1 - \\mathbf{Y}_0)\\|^2 \\mathrm{d} t\n",
    "$$\n",
    "\n",
    "**(0.1 балла)** Для обученной модели посчитайте FID (на 10к семплах) на 1-10 шагах и визуализируйте на одном графике с FID предобученной модели. Проанализируйте полученные результаты.\n",
    "\n",
    "**(0.2 балла)** начисляется, если модель бьет предобученную на 1-6 шагах.\n",
    "\n",
    "**Замечание:** качество с точки зрения FID получится не очень хорошее. Для выбивания лучшего качества нужно докручивать хаков/прилично увеличивать компьют."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d02d54e-43fa-44cb-8448-fbee1001d82b",
   "metadata": {},
   "source": [
    "Функции для подсчета FID даны ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f4762a-4af2-4d72-9eb2-7876ca874dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# сохраним семплы из модели для дальнейшего подсчета FID у FM модели\n",
    "def save_model_samples(name, model, params, batch_size, num_samples, **model_kwargs):\n",
    "    if os.path.exists(name):\n",
    "        shutil.rmtree(name)\n",
    "\n",
    "    os.makedirs(name, exist_ok=True)\n",
    "    count = 0\n",
    "\n",
    "    with tqdm(total= num_samples) as pbar:\n",
    "        while count < num_samples:\n",
    "            cur_batch_size = min(num_samples - count, batch_size)\n",
    "            noise = torch.randn(cur_batch_size, 3, 32, 32).cuda()\n",
    "\n",
    "            out, trajectory = sample_euler(model, noise, params, get_timesteps_fm, **model_kwargs)\n",
    "            out = (out * 127.5 + 128).clip(0, 255).to(torch.uint8).permute(0, 2, 3, 1).cpu().numpy()\n",
    "            for i in range(out.shape[0]):\n",
    "                img = Image.fromarray(out[i])\n",
    "                n_digits = len(str(count))\n",
    "                img_name = (6 - n_digits) * '0' + str(count) + '.png'\n",
    "                img.save(os.path.join(name, img_name))\n",
    "                count += 1\n",
    "                pbar.update(1)\n",
    "                pbar.set_description('%d images saved' % (count,))\n",
    "\n",
    "recflow.eval()\n",
    "sampling_params = {\n",
    "    'device': 'cuda',\n",
    "    'sigma_min': 0.02,\n",
    "    'sigma_max': 80.0,\n",
    "    'num_steps': 20, # на 20 итерациях FID уже практически сошелся, больше будет слишком долго учиться\n",
    "    'vis_steps': 10,\n",
    "    'rho': 7.0,\n",
    "    'stochastic': False,\n",
    "}\n",
    "save_model_samples('rf_samples', recflow, sampling_params, batch_size=128, num_samples=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a058bf-3fc1-4655-bb56-b3e2832a0b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# возьмем реализацию подсчета FID из EDM\n",
    "from fid import calculate_inception_stats, calculate_fid_from_inception_stats\n",
    "\n",
    "def calc_fid(image_path, ref_path, num_expected, batch):\n",
    "    with open_url(ref_path) as f:\n",
    "        ref = dict(np.load(f))\n",
    "\n",
    "    mu, sigma = calculate_inception_stats(image_path=image_path, num_expected=num_expected, max_batch_size=batch)\n",
    "    fid = calculate_fid_from_inception_stats(mu, sigma, ref['mu'], ref['sigma'])\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c675e-832a-4398-9fb1-68a22ec6ab0c",
   "metadata": {},
   "source": [
    "## Задача 4 (0.2 балла)\n",
    "\n",
    "**(0.1 балла)** Визуализируйте выходы предобученной модели и Rectified Flow на нескольких одинаковых входных шумах, используя 1-10 шагов в схеме Эйлера. Согласуются ли визуальные результаты с полученными значениями FID?\n",
    "\n",
    "**(0.1 балла)** Сгенерируйте батч шума и запустите генерацию по схеме Эйлера из предобученной модели и из Rectified Flow. Возьмите случайный набор пикселей из батча и визуализируйте траекторию пикселей на отрезке $[0, 1]$ (аналогично Fig.6 из статьи [InstaFlow](https://arxiv.org/pdf/2309.06380)). Оцените, насколько сильно прямее они становятся."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
